{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning a BERT model for text extraction with the SQuAD dataset\n",
    "\n",
    "We are going to fine-tune [BERT implemented by HuggingFace](https://huggingface.co/bert-base-uncased) for the text-extraction task with a dataset of questions and answers with the [SQuAD (The Stanford Question Answering Dataset)](https://rajpurkar.github.io/SQuAD-explorer/) dataset.\n",
    "The data is composed by a set of questions and corresponding paragraphs that contains the answers.\n",
    "The model will be trained to locate the answer in the context by giving the positions where the answer starts and ends.\n",
    "\n",
    "In this notebook we are going to do the training using multiple GPUs.\n",
    "\n",
    "This notebook is based on [BERT (from HuggingFace Transformers) for Text Extraction](https://keras.io/examples/nlp/text_extraction_with_bert/).\n",
    "\n",
    "More info:\n",
    "- [Glossary - HuggingFace docs](https://huggingface.co/transformers/glossary.html#model-inputs)\n",
    "- [BERT NLP â€” How To Build a Question Answering Bot](https://towardsdatascience.com/bert-nlp-how-to-build-a-question-answering-bot-98b1d1594d7b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipcmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ipcluster start -n 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pxconfig --progress-after -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "import os\n",
    "import utility.data_processing as dpp\n",
    "import utility.testing as testing\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from datasets import load_dataset, load_metric\n",
    "from datetime import datetime\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "from datasets.utils import disable_progress_bar\n",
    "from datasets import disable_caching\n",
    "\n",
    "\n",
    "disable_progress_bar()\n",
    "disable_caching()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "hf_model = 'bert-base-uncased'\n",
    "bert_cache = os.path.join(os.getcwd(), 'cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "slow_tokenizer = BertTokenizer.from_pretrained(\n",
    "    hf_model,\n",
    "    cache_dir=os.path.join(bert_cache, f'_{hf_model}-tokenizer')\n",
    ")\n",
    "save_path = os.path.join(bert_cache, f'{hf_model}-tokenizer')\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "    slow_tokenizer.save_pretrained(save_path)\n",
    "    \n",
    "# Load the fast tokenizer from saved file\n",
    "tokenizer = BertWordPieceTokenizer(os.path.join(save_path, 'vocab.txt'),\n",
    "                                   lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "model = BertForQuestionAnswering.from_pretrained(\n",
    "    hf_model,\n",
    "    cache_dir=os.path.join(bert_cache, f'{hf_model}_qa')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "from pt_distr_env import setup_distr_env\n",
    "\n",
    "setup_distr_env()\n",
    "dist.init_process_group(backend=\"nccl\")\n",
    "rank = dist.get_rank()\n",
    "world_size = dist.get_world_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "hf_dataset = load_dataset('squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "max_len = 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "hf_dataset.flatten()\n",
    "processed_dataset = hf_dataset.flatten().map(\n",
    "    lambda example: dpp.process_squad_item_batched(example, max_len, tokenizer),\n",
    "    remove_columns=hf_dataset.flatten()['train'].column_names,\n",
    "    batched=True,\n",
    "    num_proc=12\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "train_dataset = processed_dataset[\"train\"]\n",
    "train_dataset.set_format(type='torch')\n",
    "\n",
    "eval_dataset = processed_dataset[\"validation\"]\n",
    "eval_dataset.set_format(type='torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "per_device_train_batch_size = 16\n",
    "per_device_eval_batch_size = 1\n",
    "\n",
    "train_sampler = DistributedSampler(train_dataset, num_replicas=world_size,\n",
    "                                   rank=rank, shuffle=False, seed=42)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=False,  # sampler option is mutually exclusive with shuffle\n",
    "    batch_size=per_device_train_batch_size,\n",
    "    sampler=train_sampler\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset,\n",
    "    shuffle=False,\n",
    "    batch_size=per_device_eval_batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "device = 0\n",
    "model.to(device)\n",
    "model = DistributedDataParallel(model, device_ids=[device])\n",
    "model.train()\n",
    "\n",
    "model.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "for epoch in range(1):\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        optim.zero_grad()\n",
    "        outputs = model(input_ids=batch['input_ids'].to(device),\n",
    "                        token_type_ids=batch['token_type_ids'].to(device),\n",
    "                        attention_mask=batch['attention_mask'].to(device),\n",
    "                        start_positions=batch['start_token_idx'].to(device),\n",
    "                        end_positions=batch['end_token_idx'].to(device))        \n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        if i > 100:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px --target 0\n",
    "model_hash = datetime.now().strftime(\"%Y-%m-%d-%H%M%S\")\n",
    "model_path_name = f'./cache/model_trained_pytorch_{model_hash}'\n",
    "\n",
    "# save model's state_dict\n",
    "# the model now is a DDP model\n",
    "# use `model.module.state_dict()` in order the load it later on\n",
    "# any number of nodes\n",
    "torch.save(model.module.state_dict(), model_path_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px --target 0\n",
    "# create the model again since the previous one is on the gpu\n",
    "model_cpu = BertForQuestionAnswering.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    cache_dir=os.path.join(bert_cache, 'bert-base-uncased_qa')\n",
    ")\n",
    "\n",
    "# load the model on cpu\n",
    "model_cpu.load_state_dict(\n",
    "    torch.load(model_path_name,\n",
    "               map_location=torch.device('cpu'))\n",
    ")\n",
    "\n",
    "# load the model on gpu\n",
    "# model.load_state_dict(torch.load(model_path_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px --target 0\n",
    "model.eval()\n",
    "\n",
    "model.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px --target 0\n",
    "squad_example_objects = []\n",
    "for item in hf_dataset['validation'].flatten():\n",
    "    squad_examples = dpp.squad_examples_from_dataset(item, max_len, tokenizer)\n",
    "    try:\n",
    "        squad_example_objects.extend(squad_examples)\n",
    "    except TypeError:\n",
    "        squad_example_objects.append(squad_examples)\n",
    "        \n",
    "assert len(eval_dataset) == len(squad_example_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px --target 0\n",
    "\n",
    "start_sample = 0\n",
    "num_test_samples = 10\n",
    "for i, eval_batch in enumerate(eval_dataloader):\n",
    "    if i > start_sample:\n",
    "        testing.EvalUtility(eval_batch, [squad_example_objects[i]], model).results()\n",
    "\n",
    "    if i > start_sample + num_test_samples:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ipcluster stop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hpcpython2022",
   "language": "python",
   "name": "hpcpython2022"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
