{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning a BERT model for text extraction with the SQuAD dataset\n",
    "\n",
    "We have fine-tuned [BERT implemented by HuggingFace](https://huggingface.co/bert-base-uncased) for the text-extraction task with a dataset of questions and answers with the [SQuAD (The Stanford Question Answering Dataset)](https://rajpurkar.github.io/SQuAD-explorer/) dataset. Let evaluate the model.\n",
    "\n",
    "This notebook is based on [BERT (from HuggingFace Transformers) for Text Extraction](https://keras.io/examples/nlp/text_extraction_with_bert/).\n",
    "\n",
    "More info:\n",
    "- [Glossary - HuggingFace docs](https://huggingface.co/transformers/glossary.html#model-inputs)\n",
    "- [BERT NLP â€” How To Build a Question Answering Bot](https://towardsdatascience.com/bert-nlp-how-to-build-a-question-answering-bot-98b1d1594d7b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import utility.data_processing as dpp\n",
    "import utility.testing as testing\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertForQuestionAnswering\n",
    "from tokenizers import BertWordPieceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.utils import disable_progress_bar\n",
    "from datasets import disable_caching\n",
    "\n",
    "\n",
    "disable_progress_bar()\n",
    "disable_caching()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model = 'bert-base-uncased'\n",
    "bert_cache = os.path.join(os.getcwd(), 'cache')\n",
    "save_path = os.path.join(bert_cache, f'{hf_model}-tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fast tokenizer from saved file\n",
    "tokenizer = BertWordPieceTokenizer(os.path.join(save_path, 'vocab.txt'),\n",
    "                                   lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForQuestionAnswering.from_pretrained(\n",
    "    hf_model,\n",
    "    cache_dir=os.path.join(bert_cache, f'{hf_model}_qa')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_name = f'/scratch/snx3000/class350/model_trained_deepspeed_2022-07-22-145442'\n",
    "\n",
    "# load the model on cpu\n",
    "model.load_state_dict(\n",
    "    torch.load(model_path_name,\n",
    "               map_location=torch.device('cpu'))\n",
    ")\n",
    "\n",
    "# load the model on gpu\n",
    "# model.load_state_dict(torch.load(model_path_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset = load_dataset('squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = hf_dataset['validation'].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_val_ds = val_ds.map(\n",
    "    lambda example: dpp.process_squad_item_batched(example, max_len, tokenizer),\n",
    "    remove_columns=val_ds.column_names,\n",
    "    batched=True,\n",
    "    num_proc=12\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_val_ds.set_format(type='torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    processed_val_ds,\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_example_objects = []\n",
    "for item in val_ds:\n",
    "    squad_examples = dpp.squad_examples_from_dataset(item, max_len, tokenizer)\n",
    "    try:\n",
    "        squad_example_objects.extend(squad_examples)\n",
    "    except TypeError:\n",
    "        squad_example_objects.append(squad_examples)\n",
    "        \n",
    "assert len(processed_val_ds) == len(squad_example_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_sample = 24100\n",
    "num_test_samples = 10\n",
    "for i, eval_batch in enumerate(eval_dataloader):\n",
    "    if i > start_sample:\n",
    "        testing.EvalUtility(eval_batch, [squad_example_objects[i]], model).results()\n",
    "\n",
    "    if i > start_sample + num_test_samples:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hpcpython2022",
   "language": "python",
   "name": "hpcpython2022"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
