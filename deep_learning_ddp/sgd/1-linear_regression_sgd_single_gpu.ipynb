{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of a Simple Stochastic Gradient Descent\n",
    "\n",
    "Here we visualize the minimization of the loss with the SGD algorithm in its variants vanilla GD, batch SGD and minibatch SGD. For this we consider a linear model with only one weight and one bias (the slope and the offset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Let's create the dataset. We generate a random vector $x \\in [-0.5, 0.5]$ and evaluate it in a linear function $y = 2x$. We add some noise to $y$ and that give us $y \\in [-1.5, 1.5]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a linear function with noise as our data\n",
    "nsamples = 1000\n",
    "ref_slope = 2.0\n",
    "ref_offset = 0.0\n",
    "noise = np.random.random((nsamples, 1)) - 0.5    # -0.5 to center the noise\n",
    "x_train = np.random.random((nsamples, 1)) - 0.5  # -0.5 to center x around 0\n",
    "y_train = ref_slope * x_train + ref_offset + noise\n",
    "\n",
    "plt.plot(x_train, y_train, '.')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((x_train.astype(np.float32),\n",
    "                                              y_train.astype(np.float32)))\n",
    "dataset = dataset.shuffle(1000)  # shuffle by chunks of 1000 samples. In this case, that's the how dataset.\n",
    "dataset = dataset.batch(100)     # set the batch size\n",
    "dataset = dataset.repeat(150)    # set the number of epocs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model and choosing an optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(1, input_shape=(1,), activation='linear'),\n",
    "])\n",
    "\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=0.5)\n",
    "\n",
    "model.compile(optimizer=opt,\n",
    "              loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a callback to store the training history\n",
    "This is not really part of the training. We store the SGD trajectory for plotting later. What this does, is to append the current values of the wights and the loss after every batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainHistory(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.vars = []\n",
    "        self.loss = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        _slope, _offset = [v.numpy() for v in self.model.variables]\n",
    "        self.vars.append([_slope[0, 0], _offset[0]])\n",
    "        self.loss.append(logs.get('loss'))\n",
    "\n",
    "\n",
    "history = TrainHistory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = model.fit(dataset, callbacks=[history])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope_hist = np.array(history.vars)[:, 0]\n",
    "offset_hist = np.array(history.vars)[:, 1]\n",
    "loss_hist = np.array(history.loss)\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (10, 4)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(loss_hist[10:], 'r.-')\n",
    "plt.xlabel('Training steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x_train, y_train, '.', alpha=.3)\n",
    "plt.plot(x_train, slope_hist[0]  * x_train + offset_hist[0],  'r-', label='model (initial step)')\n",
    "plt.plot(x_train, slope_hist[-1] * x_train + offset_hist[-1], 'g-', label='model (trained)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (6, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function_field(m, n, xref, yref):\n",
    "    '''Utility function for ploting the loss'''\n",
    "    return np.mean(np.square(yref - m * xref - n ))\n",
    "\n",
    "_m = np.arange(-0, 4.01, 0.1)\n",
    "_n = np.arange(-0.5, 0.51, 0.1)\n",
    "M, N = np.meshgrid(_m, _n)\n",
    "\n",
    "Z = np.zeros(M.shape)\n",
    "for i in range(M.shape[0]):\n",
    "    for j in range(M.shape[1]):\n",
    "        Z[i, j] = loss_function_field(M[i, j], N[i, j],\n",
    "                                      x_train, y_train)\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (10.0, 7.0)\n",
    "\n",
    "cp = plt.contour(M, N, Z, 15, vmin=Z.min(), vmax=Z.max(), alpha=0.99, colors='k', linestyles='--')\n",
    "plt.contourf(M, N, Z, vmin=Z.min(), vmax=Z.max(), alpha=0.8, cmap=plt.cm.RdYlBu_r)\n",
    "plt.clabel(cp, cp.levels[:6])\n",
    "plt.colorbar()\n",
    "m = slope_hist[-1]\n",
    "n = offset_hist[-1]\n",
    "plt.plot(slope_hist, offset_hist, '.-', lw=2, c='k')\n",
    "plt.plot([ref_slope], [ref_offset], 'rx', ms=10)\n",
    "plt.xlim([_m.min(), _m.max()])\n",
    "plt.ylim([_n.min(), _n.max()])\n",
    "plt.xlabel('Slope')\n",
    "plt.ylabel('Offset')\n",
    "plt.show()\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (6, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>Exercise 1</mark>: Try different batch sizes:\n",
    " * `batch_size = 1000` (The whole dataset)\n",
    " * `batch_size = 100` (Something between 1 and the size of whole dataset.)\n",
    " * `batch_size = 1`\n",
    "\n",
    "1. Check the SGD trajectories. Why are they different?\n",
    "2. The case `batch_size = 1` is quite slow compared to `batch_size = 100` and `batch_size = 1000`. Why?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-su2022",
   "language": "python",
   "name": "tf-su2022"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
