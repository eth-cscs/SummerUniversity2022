{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the Euclidean Distance Matrix with NumPy\n",
    "\n",
    "In this notebook we implement two functions to compute the Euclidean distance matrix. We use a simple algebra trick that makes possible to write the function in a completely vectorized way in terms of optimized NumPy functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know bradcasting, let's use it to implement a function that calculates the Euclidean distance matrix of an array of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_broadcast(x, y):\n",
    "    \"\"\"Euclidean square distance matrix.\n",
    "    \n",
    "    Inputs:\n",
    "    x: (N, m) numpy array\n",
    "    y: (N, m) numpy array\n",
    "    \n",
    "    Ouput:\n",
    "    (N, N) Euclidean square distance matrix:\n",
    "    r_ij = (x_ij - y_ij)^2\n",
    "    \"\"\"\n",
    "    diff = x[:, np.newaxis, :] - y[np.newaxis, :, :]\n",
    "\n",
    "    return (diff * diff).sum(axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark> Question </mark>: At this point you are starting to get acquainted with the `numpy.ndarray`s and it's memory managment. Could you analyse advantages and possible drawbacks of the `euclidean_broadcast` function? Write a positive and a negative point about it.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider now a more sophisticated implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_trick(x, y):\n",
    "    \"\"\"Euclidean square distance matrix.\n",
    "    \n",
    "    Inputs:\n",
    "    x: (N, m) numpy array\n",
    "    y: (N, m) numpy array\n",
    "    \n",
    "    Ouput:\n",
    "    (N, N) Euclidean square distance matrix:\n",
    "    r_ij = (x_ij - y_ij)^2\n",
    "    \"\"\"\n",
    "    x2 = (x*x).sum(axis=1)[:, np.newaxis]\n",
    "    y2 = (y*y).sum(axis=1)[np.newaxis, :]\n",
    "\n",
    "    xy = x @ y.T\n",
    "\n",
    "    return np.abs(x2 + y2 - 2. * xy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `euclidean_trick` function\n",
    "\n",
    "Each element of the Euclidean distance matrix is the scalar product of the difference between two rows of the array. `euclidean_trick` takes advantage of this by doing the following\n",
    "$$\n",
    "\\sum_k {(x_{ik}-y_{ik})^2} = (\\vec{x}_i - \\vec{y}_j)\\cdot(\\vec{x}_i - \\vec{y}_j) = \\vec{x}_i\\cdot\\vec{x}_i + \\vec{y}_j\\cdot\\vec{y}_j - 2\\vec{x}_i\\cdot\\vec{y}_j\n",
    "$$\n",
    "\n",
    "Fortunately, there are NumPy functions to compute each of these terms:\n",
    "\n",
    "$\\vec{x}_i\\cdot\\vec{y}_j$ $\\rightarrow$ `x @ y.T` : Matrix product of $\\{\\vec{x}\\}$ and $\\{\\vec{y}\\}$\n",
    "\n",
    "$\\vec{x}_i\\cdot\\vec{x}_i$ $\\rightarrow$ `(x*x).sum(axis=1)[:, np.newaxis]` : A $(n,1)$ vector of elements $\\sum_j x_{ij}x_{ij}$\n",
    "\n",
    "$\\vec{y}_j\\cdot\\vec{y}_j$ $\\rightarrow$ `(y*y).sum(axis=1)[:, np.newaxis]` : A $(1,n)$ vector of elements $\\sum_j y_{ij}y_{ij}$\n",
    "\n",
    "To have all the combinations $ij$ of the sum $\\vec{x}_i\\cdot\\vec{x}_i + \\vec{y}_j\\cdot\\vec{y}_j$, we add a new axis to each of the arrays, transpose one them and add them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use `@` to perform the matrix multiplication of the full dataset by itself. We didn't use it before as alternative to `(x*x).sum(axis=1)` because it doesn't perform row by row scalar products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamples = 10\n",
    "nfeat = 3\n",
    "\n",
    "x = 10. * np.random.random([nsamples, nfeat])\n",
    "\n",
    "xy = x @ x.T\n",
    "xy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's time them and look at the `top` command to see how `@` uses multiple OpenMP threads. Let's check also that they give the same result ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamples = 2000\n",
    "nfeat = 50\n",
    "\n",
    "x = 10. * np.random.random([nsamples, nfeat])\n",
    "\n",
    "%timeit euclidean_broadcast(x, x)\n",
    "%timeit euclidean_trick(x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.abs(euclidean_broadcast(x, x) - euclidean_trick(x, x)).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "False in np.isclose(euclidean_broadcast(x, x), euclidean_trick(x, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let use `line_profiler` to time every line of our functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -f euclidean_trick euclidean_trick(x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -f euclidean_broadcast euclidean_broadcast(x, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "The main points to take from this notebook are:\n",
    "  * NumPy is all about vectorization. Loops in python must be avoided.\n",
    "  * Always consider different vectorized implementations and compare them.\n",
    "  * Even within NumPy, some functions might bring a more significant speedup than others.\n",
    "  \n",
    "> To get an extra speed up, we can use `np.einsum('ij,ij->i', x, x)` to compute the terms $\\vec{x}_i\\cdot\\vec{x}_i$ and $\\vec{y}_i\\cdot\\vec{y}_i$instead of `(x*x)sum(axis=1)`. Please. have a look to the notebook `numpy/04-euclidean-distance-matrix-numpy-advanced.ipynb`. If there are questions, we can discuss it on the Q&A!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hpcpython2022",
   "language": "python",
   "name": "hpcpython2022"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
